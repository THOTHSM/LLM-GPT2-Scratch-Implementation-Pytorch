{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip3 install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7oCkEGUMHZVP",
        "outputId": "5cbe5920-510e-4186-be17-65beb2c87ed5"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "gvvrSB8oFdnr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Multi_Head_Attention(torch.nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout_rate, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert (d_out % num_heads == 0),\"d_out must be divisible by num_heads\"\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads\n",
        "\n",
        "        self.W_query = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "\n",
        "        self.Dropout = torch.nn.Dropout(dropout_rate)\n",
        "\n",
        "        self.register_buffer(\"mask\",torch.triu(torch.ones(context_length, context_length),diagonal=1))\n",
        "\n",
        "    def forward(self,x):\n",
        "        batch , number_of_tokens , d_in = x.shape\n",
        "\n",
        "        self.query = self.W_query(x)\n",
        "        self.key = self.W_key(x)\n",
        "        self.value = self.W_value(x)\n",
        "\n",
        "        query = self.query.view(batch,number_of_tokens,self.num_heads,self.head_dim) #Adjusting size for all heads\n",
        "        key = self.key.view(batch,number_of_tokens,self.num_heads,self.head_dim)\n",
        "        value = self.value.view(batch,number_of_tokens,self.num_heads,self.head_dim)\n",
        "\n",
        "        key = key.transpose(1, 2)\n",
        "        query = query.transpose(1, 2)\n",
        "        value = value.transpose(1, 2)\n",
        "\n",
        "        attention_score = query @ key.transpose(2,3)\n",
        "\n",
        "         # Converting to true or false matrix and shaping to number of tokens\n",
        "        mask_bool = self.mask.bool()[:number_of_tokens, :number_of_tokens]\n",
        "\n",
        "        attention_score.masked_fill_(mask_bool,-torch.inf)\n",
        "\n",
        "        attention_weights = torch.softmax(attention_score/self.head_dim**0.5,dim=-1)\n",
        "\n",
        "        attention_weights = self.Dropout(attention_weights)\n",
        "\n",
        "        context_vector = (attention_weights @ value).transpose(1,2)\n",
        "\n",
        "        context_vector = context_vector.contiguous().view(batch,number_of_tokens,self.d_out)\n",
        "        return context_vector"
      ],
      "metadata": {
        "id": "PBHev9iKF1Dg"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer_normalization(torch.nn.Module):\n",
        "    def __init__(self,emb_dim):\n",
        "        super().__init__()\n",
        "        self.epsilon = 1e-5\n",
        "        self.scale = torch.nn.Parameter(torch.ones(emb_dim))\n",
        "        self.shift = torch.nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "    def forward(self,x):\n",
        "        mean = x.mean(dim=-1,keepdim=True)\n",
        "        variance = x.var(dim=-1,keepdim=True,unbiased=False)\n",
        "        norm_x = (x-mean)/torch.sqrt(variance+self.epsilon)\n",
        "        return self.scale*norm_x+self.shift"
      ],
      "metadata": {
        "id": "fSw8g7MUGJnf"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Gelu(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self,x): # GELU activation function:\n",
        "        # f(x) = 0.5 * x * (1 + tanh(sqrt(2/pi) * (x + 0.044715 * x^3)))\n",
        "        return 0.5*x*(1+torch.tanh(torch.sqrt(torch.tensor(2.0/torch.pi))*(x+0.044715*torch.pow(x,3))))"
      ],
      "metadata": {
        "id": "o3uYLquMGMk-"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Feed_forward(torch.nn.Module):\n",
        "    def __init__(self,cfg):\n",
        "        super().__init__()\n",
        "        self.layers = torch.nn.Sequential(\n",
        "            torch.nn.Linear(cfg[\"emb_dim\"],4*cfg[\"emb_dim\"]),\n",
        "            Gelu(),\n",
        "            torch.nn.Linear(4*cfg[\"emb_dim\"],cfg[\"emb_dim\"])\n",
        "        )\n",
        "    def forward(self,x):\n",
        "        return self.layers(x)"
      ],
      "metadata": {
        "id": "VX749HksGN32"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(torch.nn.Module):\n",
        "    def __init__(self,cfg):\n",
        "        super().__init__()\n",
        "\n",
        "        self.att = Multi_Head_Attention( d_in=cfg[\"emb_dim\"],\n",
        "                                        d_out=cfg[\"emb_dim\"],\n",
        "                                        context_length=cfg[\"context_length\"],\n",
        "                                        dropout_rate=cfg[\"drop_rate\"],\n",
        "                                        num_heads=cfg[\"n_heads\"],\n",
        "                                        qkv_bias=cfg[\"qkv_bias\"])\n",
        "\n",
        "        self.layer_norm1 = Layer_normalization(emb_dim=cfg[\"emb_dim\"])\n",
        "\n",
        "        self.layer_norm2 = Layer_normalization(emb_dim=cfg[\"emb_dim\"])\n",
        "\n",
        "        self.feed_forward = Feed_forward(cfg=cfg)\n",
        "\n",
        "        self.dropout = torch.nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "    def forward(self,x):\n",
        "        shortcut =x\n",
        "        x = self.layer_norm1(x)\n",
        "        x = self.att(x)\n",
        "        x = self.dropout(x)\n",
        "        x = x+shortcut\n",
        "        shortcut = x\n",
        "        x = self.layer_norm2(x)\n",
        "        x = self.feed_forward(x)\n",
        "        x = self.dropout(x)\n",
        "        x = x + shortcut\n",
        "        return x\n",
        ""
      ],
      "metadata": {
        "id": "UpMWkCi6GXJO"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTModel(torch.nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "\n",
        "        self.tok_emb = torch.nn.Embedding(cfg[\"vocab_size\"],cfg[\"emb_dim\"])\n",
        "        self.pos_emb = torch.nn.Embedding(cfg[\"context_length\"],cfg[\"emb_dim\"])\n",
        "        self.drop_emb = torch.nn.Dropout(cfg[\"drop_rate\"])\n",
        "        self.transformer_blocks = torch.nn.Sequential(*[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
        "        self.final_norm = Layer_normalization(cfg[\"emb_dim\"])\n",
        "        self.output_head = torch.nn.Linear(cfg[\"emb_dim\"],cfg[\"vocab_size\"],bias=False)\n",
        "\n",
        "    def forward(self,id_idx):\n",
        "        batch,seq_len = id_idx.shape\n",
        "        token_embeded = self.tok_emb(id_idx)\n",
        "        position_embeded = self.pos_emb(torch.arange(seq_len, device= id_idx.device))\n",
        "        x = token_embeded + position_embeded\n",
        "        x = self.drop_emb(x)\n",
        "        x = self.transformer_blocks(x)\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.output_head(x)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "lHNoaYmIGjzG"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,   # Vocabulary size\n",
        "    \"context_length\": 1024, # Shortened context length (orig: 1024)\n",
        "    \"emb_dim\": 768,        # Embedding dimension\n",
        "    \"n_heads\": 12,         # Number of attention heads\n",
        "    \"n_layers\": 12,        # Number of layers\n",
        "    \"drop_rate\": 0.1,      # Dropout rate\n",
        "    \"qkv_bias\": False      # Query-key-value bias\n",
        "}\n"
      ],
      "metadata": {
        "id": "AXWdXDLjLC9R"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPTModel(GPT_CONFIG_124M)"
      ],
      "metadata": {
        "id": "Ml7YfPIOK7-B"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CategoricalCrossEntropy:\n",
        "    def __init__(self, target):\n",
        "        self.target_flat = target.flatten()\n",
        "\n",
        "    def loss(self, logits_matrix):\n",
        "        logits_flat = logits_matrix.view(-1,logits_matrix.shape[-1])\n",
        "        log_probs = torch.log_softmax(logits_flat, dim=-1)\n",
        "        selected_log_probs = log_probs[torch.arange(logits_flat.size(0)),self.target_flat]\n",
        "        self.loss = -torch.sum(selected_log_probs) /logits_matrix.shape[0]\n",
        "        return self.loss\n",
        "\n",
        "    def perplexity(self):\n",
        "        return torch.exp(self.loss)"
      ],
      "metadata": {
        "id": "ZGWKWm54G40G"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"Fire & Blood by George R R Martin.txt\", \"r\", encoding=\"utf-8\") as file:\n",
        "        text_data = file.read()"
      ],
      "metadata": {
        "id": "BD1KIqhWG_F3"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text_data[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zj6psS1gHImH",
        "outputId": "f7fd4ce3-4515-4786-9330-eb407f55e910"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\f\fFire & Blood is a \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Charector : {len(text_data)}\")\n",
        "tokenizer=tiktoken.get_encoding(\"gpt2\")\n",
        "tokens = tokenizer.encode(text_data)\n",
        "print(f\"Token  : {len(tokens)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTPgVxI6HLQn",
        "outputId": "02229a0a-ae15-422c-aa45-1c3243edfae0"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Charector : 1444998\n",
            "Token  : 379444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset ,DataLoader\n",
        "\n",
        "class GPTDataset(Dataset):\n",
        "    def __init__(self,text_data,tokenizer,max_length,stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        tokens = tokenizer.encode(text_data, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "        for i in range(0,len(tokens)-max_length,stride):\n",
        "\n",
        "            input_chunk = tokens[i:i+max_length]\n",
        "            target_chunk = tokens[i+1:i+max_length+1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return  self.input_ids[index],self.target_ids[index]\n",
        "\n",
        "\n",
        "def dataloader_dataset(text_data, batch_size=4, max_length=256,\n",
        "                         stride=128, shuffle=True, drop_last=True,\n",
        "                         num_workers=0):\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "    dataset = GPTDataset(text_data,tokenizer,max_length,stride)\n",
        "    dataloader = DataLoader(dataset,batch_size=batch_size,shuffle=shuffle,drop_last=drop_last,num_workers=num_workers)\n",
        "    return dataloader"
      ],
      "metadata": {
        "id": "UiefSUGfIPk3"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trian_percent = 0.8\n",
        "index_train_val = int(len(text_data)*trian_percent)\n",
        "\n",
        "train_data = text_data[:index_train_val]\n",
        "\n",
        "val_data  = text_data[index_train_val:]\n",
        "print(len(train_data),len(val_data))\n",
        "\n",
        "train_data_loader = dataloader_dataset(train_data,batch_size=2,max_length=GPT_CONFIG_124M[\"context_length\"],\n",
        "                                          stride=GPT_CONFIG_124M[\"context_length\"],drop_last=True,shuffle=True,num_workers=0)\n",
        "\n",
        "val_data_loader = dataloader_dataset(val_data,batch_size=2,max_length=GPT_CONFIG_124M[\"context_length\"],\n",
        "                                          stride=GPT_CONFIG_124M[\"context_length\"],drop_last=True,shuffle=True,num_workers=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpfiRD2dIU1n",
        "outputId": "e525bafa-7f73-4fa1-bd12-3e6eb6635d32"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1155998 289000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train loader:\")\n",
        "for x, y in train_data_loader:\n",
        "    print(x.shape, y.shape)\n",
        "\n",
        "print(\"\\nValidation loader:\")\n",
        "for x, y in val_data_loader:\n",
        "    print(x.shape, y.shape)\n",
        "\n",
        "print(len(train_data_loader))\n",
        "print(len(val_data_loader))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTMi_5ewIcen",
        "outputId": "43b74005-e547-4453-84ee-74396819e0bf"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loader:\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "\n",
            "Validation loader:\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
            "148\n",
            "37\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "    logits = model(input_batch)\n",
        "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
        "    return loss\n",
        "\n",
        "\n",
        "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
        "    total_loss = 0.\n",
        "    if len(data_loader) == 0:\n",
        "        return float(\"nan\")\n",
        "    elif num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "        # Reduce the number of batches to match the total number of batches in the data loader\n",
        "        # if num_batches exceeds the number of batches in the data loader\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i < num_batches:\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            total_loss += loss.item()\n",
        "        else:\n",
        "            break\n",
        "    return total_loss / num_batches"
      ],
      "metadata": {
        "id": "M760X2cBIh73"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Note:\n",
        "# Uncommenting the following lines will allow the code to run on Apple Silicon chips, if applicable,\n",
        "# which is approximately 2x faster than on an Apple CPU (as measured on an M3 MacBook Air).\n",
        "# However, the resulting loss values may be slightly different.\n",
        "\n",
        "#if torch.cuda.is_available():\n",
        "#    device = torch.device(\"cuda\")\n",
        "#elif torch.backends.mps.is_available():\n",
        "#    device = torch.device(\"mps\")\n",
        "#else:\n",
        "#    device = torch.device(\"cpu\")\n",
        "#\n",
        "# print(f\"Using {device} device.\")\n",
        "\n",
        "\n",
        "model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n",
        "\n",
        "\n",
        "torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader\n",
        "\n",
        "with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
        "    train_loss = calc_loss_loader(train_data_loader, model, device)\n",
        "    val_loss = calc_loss_loader(val_data_loader, model, device)\n",
        "\n",
        "print(\"Training loss:\", train_loss)\n",
        "print(\"Validation loss:\", val_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOfX5C6NIjXP",
        "outputId": "99f04e9c-50ec-4315-e1f6-a07b006e8e04"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 10.986816883087158\n",
            "Validation loss: 10.987972568821263\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mxIIlEnPLJCu",
        "outputId": "3bbe1e92-e7c8-4853-bc71-29d51f4e6cef"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
        "\n",
        "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n",
        "\n",
        "        idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "    return idx"
      ],
      "metadata": {
        "id": "kTOHjoOyMWb6"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
        "    if device is not None:\n",
        "        idx = idx.to(device)\n",
        "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # New: Filter logits with top_k sampling\n",
        "        if top_k is not None:\n",
        "            # Keep only top_k values\n",
        "            top_logits, _ = torch.topk(logits, top_k)\n",
        "            min_val = top_logits[:, -1]\n",
        "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
        "\n",
        "        # New: Apply temperature scaling\n",
        "        if temperature > 0.0:\n",
        "            logits = logits / temperature\n",
        "\n",
        "            # Apply softmax to get probabilities\n",
        "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
        "\n",
        "            # Sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
        "\n",
        "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
        "        else:\n",
        "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
        "\n",
        "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
        "            break\n",
        "\n",
        "        # Same as before: append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
        "\n",
        "    return idx"
      ],
      "metadata": {
        "id": "jesDKY46MQbG"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_token_ids(text, tokenizer):\n",
        "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
        "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
        "    return encoded_tensor\n",
        "\n",
        "def token_ids_to_text(token_ids, tokenizer):\n",
        "    flat = token_ids.squeeze(0) # remove batch dimension\n",
        "    return tokenizer.decode(flat.tolist())"
      ],
      "metadata": {
        "id": "nPJSvgReL7oh"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
        "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
        "    model.train()\n",
        "    return train_loss, val_loss"
      ],
      "metadata": {
        "id": "sB77mhmnLYbR"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
        "    model.eval()\n",
        "    context_size = model.pos_emb.weight.shape[0]\n",
        "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
        "    with torch.no_grad():\n",
        "        token_ids = generate(model=model,idx=text_to_token_ids(\"Targaryen dynasty's\", tokenizer),\n",
        "                              max_new_tokens=15,context_size=GPT_CONFIG_124M[\"context_length\"],top_k=25,\n",
        "                                temperature=1.4)\n",
        "\n",
        "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
        "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
        "    model.train()"
      ],
      "metadata": {
        "id": "MKVQE9FnLZ4y"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
        "                       eval_freq, eval_iter, start_context, tokenizer):\n",
        "    # Initialize lists to track losses and tokens seen\n",
        "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
        "    tokens_seen, global_step = 0, -1\n",
        "\n",
        "    # Main training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "\n",
        "        for input_batch, target_batch in train_loader:\n",
        "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            loss.backward() # Calculate loss gradients\n",
        "            optimizer.step() # Update model weights using loss gradients\n",
        "            tokens_seen += input_batch.numel() # Returns the total number of elements (or tokens) in the input_batch.\n",
        "            global_step += 1\n",
        "\n",
        "            # Optional evaluation step\n",
        "            if global_step % eval_freq == 0:\n",
        "                train_loss, val_loss = evaluate_model(\n",
        "                    model, train_loader, val_loader, device, eval_iter)\n",
        "                train_losses.append(train_loss)\n",
        "                val_losses.append(val_loss)\n",
        "                track_tokens_seen.append(tokens_seen)\n",
        "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
        "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
        "\n",
        "        # Print a sample text after each epoch\n",
        "        generate_and_print_sample(\n",
        "            model, tokenizer, device, start_context\n",
        "        )\n",
        "\n",
        "    return train_losses, val_losses, track_tokens_seen"
      ],
      "metadata": {
        "id": "Tnyr1mYiMf0D"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.1)\n",
        "\n",
        "num_epochs = 15\n",
        "train_losses, val_losses, tokens_seen = train_model_simple(\n",
        "    model, train_data_loader, val_data_loader, optimizer, device,\n",
        "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
        "    start_context=\"Targaryen dynasty's\", tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "# Note:\n",
        "# Uncomment the following code to show the execution time\n",
        "end_time = time.time()\n",
        "execution_time_minutes = (end_time - start_time) / 60\n",
        "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ky2eWM_Mn2a",
        "outputId": "ec59666f-f426-40e1-e1de-6252571212c3"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 1 (Step 000000): Train loss 9.640, Val loss 9.709\n",
            "Ep 1 (Step 000005): Train loss 7.318, Val loss 7.465\n",
            "Ep 1 (Step 000010): Train loss 7.253, Val loss 7.455\n",
            "Ep 1 (Step 000015): Train loss 7.051, Val loss 7.323\n",
            "Ep 1 (Step 000020): Train loss 7.011, Val loss 7.027\n",
            "Ep 1 (Step 000025): Train loss 6.728, Val loss 6.840\n",
            "Ep 1 (Step 000030): Train loss 6.567, Val loss 6.834\n",
            "Ep 1 (Step 000035): Train loss 6.404, Val loss 6.579\n",
            "Ep 1 (Step 000040): Train loss 6.382, Val loss 6.463\n",
            "Ep 1 (Step 000045): Train loss 6.191, Val loss 6.513\n",
            "Ep 1 (Step 000050): Train loss 6.248, Val loss 6.345\n",
            "Ep 1 (Step 000055): Train loss 6.019, Val loss 6.375\n",
            "Ep 1 (Step 000060): Train loss 6.030, Val loss 6.474\n",
            "Ep 1 (Step 000065): Train loss 5.953, Val loss 6.293\n",
            "Ep 1 (Step 000070): Train loss 5.911, Val loss 6.277\n",
            "Ep 1 (Step 000075): Train loss 5.822, Val loss 6.240\n",
            "Ep 1 (Step 000080): Train loss 6.016, Val loss 6.254\n",
            "Ep 1 (Step 000085): Train loss 5.830, Val loss 6.105\n",
            "Ep 1 (Step 000090): Train loss 5.769, Val loss 6.185\n",
            "Ep 1 (Step 000095): Train loss 5.728, Val loss 6.106\n",
            "Ep 1 (Step 000100): Train loss 5.612, Val loss 6.059\n",
            "Ep 1 (Step 000105): Train loss 5.578, Val loss 6.084\n",
            "Ep 1 (Step 000110): Train loss 5.549, Val loss 6.057\n",
            "Ep 1 (Step 000115): Train loss 5.526, Val loss 5.952\n",
            "Ep 1 (Step 000120): Train loss 5.665, Val loss 5.949\n",
            "Ep 1 (Step 000125): Train loss 5.620, Val loss 5.921\n",
            "Ep 1 (Step 000130): Train loss 5.336, Val loss 5.868\n",
            "Ep 1 (Step 000135): Train loss 5.427, Val loss 6.024\n",
            "Ep 1 (Step 000140): Train loss 5.459, Val loss 5.820\n",
            "Ep 1 (Step 000145): Train loss 5.396, Val loss 5.815\n",
            "Targaryen dynasty's. Har have have come of the castle from the Citadel who A\n",
            "Ep 2 (Step 000150): Train loss 5.285, Val loss 5.632\n",
            "Ep 2 (Step 000155): Train loss 5.376, Val loss 5.886\n",
            "Ep 2 (Step 000160): Train loss 5.342, Val loss 5.952\n",
            "Ep 2 (Step 000165): Train loss 5.124, Val loss 5.892\n",
            "Ep 2 (Step 000170): Train loss 5.218, Val loss 5.740\n",
            "Ep 2 (Step 000175): Train loss 5.320, Val loss 5.878\n",
            "Ep 2 (Step 000180): Train loss 5.081, Val loss 5.803\n",
            "Ep 2 (Step 000185): Train loss 5.118, Val loss 5.729\n",
            "Ep 2 (Step 000190): Train loss 5.134, Val loss 5.764\n",
            "Ep 2 (Step 000195): Train loss 5.126, Val loss 5.579\n",
            "Ep 2 (Step 000200): Train loss 5.104, Val loss 5.726\n",
            "Ep 2 (Step 000205): Train loss 5.155, Val loss 5.718\n",
            "Ep 2 (Step 000210): Train loss 4.951, Val loss 5.665\n",
            "Ep 2 (Step 000215): Train loss 5.051, Val loss 5.586\n",
            "Ep 2 (Step 000220): Train loss 4.976, Val loss 5.617\n",
            "Ep 2 (Step 000225): Train loss 4.922, Val loss 5.581\n",
            "Ep 2 (Step 000230): Train loss 4.996, Val loss 5.783\n",
            "Ep 2 (Step 000235): Train loss 5.006, Val loss 5.498\n",
            "Ep 2 (Step 000240): Train loss 4.939, Val loss 5.710\n",
            "Ep 2 (Step 000245): Train loss 5.011, Val loss 5.639\n",
            "Ep 2 (Step 000250): Train loss 4.981, Val loss 5.765\n",
            "Ep 2 (Step 000255): Train loss 4.907, Val loss 5.424\n",
            "Ep 2 (Step 000260): Train loss 4.962, Val loss 5.527\n",
            "Ep 2 (Step 000265): Train loss 4.811, Val loss 5.501\n",
            "Ep 2 (Step 000270): Train loss 4.882, Val loss 5.534\n",
            "Ep 2 (Step 000275): Train loss 4.881, Val loss 5.459\n",
            "Ep 2 (Step 000280): Train loss 5.061, Val loss 5.527\n",
            "Ep 2 (Step 000285): Train loss 4.925, Val loss 5.532\n",
            "Ep 2 (Step 000290): Train loss 4.842, Val loss 5.594\n",
            "Ep 2 (Step 000295): Train loss 4.932, Val loss 5.284\n",
            "Targaryen dynasty's, and he A few, he was no time of the next day\n",
            "Ep 3 (Step 000300): Train loss 4.815, Val loss 5.518\n",
            "Ep 3 (Step 000305): Train loss 4.854, Val loss 5.561\n",
            "Ep 3 (Step 000310): Train loss 4.676, Val loss 5.471\n",
            "Ep 3 (Step 000315): Train loss 4.600, Val loss 5.597\n",
            "Ep 3 (Step 000320): Train loss 4.732, Val loss 5.728\n",
            "Ep 3 (Step 000325): Train loss 4.731, Val loss 5.464\n",
            "Ep 3 (Step 000330): Train loss 4.814, Val loss 5.501\n",
            "Ep 3 (Step 000335): Train loss 4.768, Val loss 5.475\n",
            "Ep 3 (Step 000340): Train loss 4.728, Val loss 5.549\n",
            "Ep 3 (Step 000345): Train loss 4.757, Val loss 5.499\n",
            "Ep 3 (Step 000350): Train loss 4.515, Val loss 5.519\n",
            "Ep 3 (Step 000355): Train loss 4.730, Val loss 5.592\n",
            "Ep 3 (Step 000360): Train loss 4.772, Val loss 5.392\n",
            "Ep 3 (Step 000365): Train loss 4.905, Val loss 5.628\n",
            "Ep 3 (Step 000370): Train loss 4.911, Val loss 5.631\n",
            "Ep 3 (Step 000375): Train loss 4.730, Val loss 5.423\n",
            "Ep 3 (Step 000380): Train loss 4.641, Val loss 5.427\n",
            "Ep 3 (Step 000385): Train loss 4.598, Val loss 5.453\n",
            "Ep 3 (Step 000390): Train loss 4.537, Val loss 5.414\n",
            "Ep 3 (Step 000395): Train loss 4.620, Val loss 5.435\n",
            "Ep 3 (Step 000400): Train loss 4.525, Val loss 5.271\n",
            "Ep 3 (Step 000405): Train loss 4.609, Val loss 5.379\n",
            "Ep 3 (Step 000410): Train loss 4.611, Val loss 5.405\n",
            "Ep 3 (Step 000415): Train loss 4.660, Val loss 5.429\n",
            "Ep 3 (Step 000420): Train loss 4.608, Val loss 5.435\n",
            "Ep 3 (Step 000425): Train loss 4.645, Val loss 5.375\n",
            "Ep 3 (Step 000430): Train loss 4.622, Val loss 5.405\n",
            "Ep 3 (Step 000435): Train loss 4.459, Val loss 5.427\n",
            "Ep 3 (Step 000440): Train loss 4.533, Val loss 5.348\n",
            "Targaryen dynasty's�off and Princess Rhaenyra her father and the prince�\n",
            "Ep 4 (Step 000445): Train loss 4.567, Val loss 5.450\n",
            "Ep 4 (Step 000450): Train loss 4.454, Val loss 5.415\n",
            "Ep 4 (Step 000455): Train loss 4.502, Val loss 5.493\n",
            "Ep 4 (Step 000460): Train loss 4.567, Val loss 5.322\n",
            "Ep 4 (Step 000465): Train loss 4.430, Val loss 5.460\n",
            "Ep 4 (Step 000470): Train loss 4.404, Val loss 5.376\n",
            "Ep 4 (Step 000475): Train loss 4.553, Val loss 5.390\n",
            "Ep 4 (Step 000480): Train loss 4.355, Val loss 5.346\n",
            "Ep 4 (Step 000485): Train loss 4.330, Val loss 5.590\n",
            "Ep 4 (Step 000490): Train loss 4.495, Val loss 5.340\n",
            "Ep 4 (Step 000495): Train loss 4.543, Val loss 5.396\n",
            "Ep 4 (Step 000500): Train loss 4.370, Val loss 5.530\n",
            "Ep 4 (Step 000505): Train loss 4.459, Val loss 5.518\n",
            "Ep 4 (Step 000510): Train loss 4.362, Val loss 5.264\n",
            "Ep 4 (Step 000515): Train loss 4.431, Val loss 5.433\n",
            "Ep 4 (Step 000520): Train loss 4.230, Val loss 5.306\n",
            "Ep 4 (Step 000525): Train loss 4.444, Val loss 5.463\n",
            "Ep 4 (Step 000530): Train loss 4.180, Val loss 5.510\n",
            "Ep 4 (Step 000535): Train loss 4.302, Val loss 5.434\n",
            "Ep 4 (Step 000540): Train loss 4.417, Val loss 5.355\n",
            "Ep 4 (Step 000545): Train loss 4.339, Val loss 5.323\n",
            "Ep 4 (Step 000550): Train loss 4.338, Val loss 5.345\n",
            "Ep 4 (Step 000555): Train loss 4.310, Val loss 5.356\n",
            "Ep 4 (Step 000560): Train loss 4.207, Val loss 5.374\n",
            "Ep 4 (Step 000565): Train loss 4.298, Val loss 5.555\n",
            "Ep 4 (Step 000570): Train loss 4.414, Val loss 5.301\n",
            "Ep 4 (Step 000575): Train loss 4.281, Val loss 5.439\n",
            "Ep 4 (Step 000580): Train loss 4.299, Val loss 5.221\n",
            "Ep 4 (Step 000585): Train loss 4.167, Val loss 5.337\n",
            "Ep 4 (Step 000590): Train loss 4.286, Val loss 5.413\n",
            "Targaryen dynasty's. his daughter,’s army were The king, a\n",
            "Ep 5 (Step 000595): Train loss 4.277, Val loss 5.399\n",
            "Ep 5 (Step 000600): Train loss 4.279, Val loss 5.456\n",
            "Ep 5 (Step 000605): Train loss 4.180, Val loss 5.305\n",
            "Ep 5 (Step 000610): Train loss 4.035, Val loss 5.433\n",
            "Ep 5 (Step 000615): Train loss 4.346, Val loss 5.526\n",
            "Ep 5 (Step 000620): Train loss 4.202, Val loss 5.374\n",
            "Ep 5 (Step 000625): Train loss 4.113, Val loss 5.313\n",
            "Ep 5 (Step 000630): Train loss 4.038, Val loss 5.475\n",
            "Ep 5 (Step 000635): Train loss 4.096, Val loss 5.470\n",
            "Ep 5 (Step 000640): Train loss 4.308, Val loss 5.667\n",
            "Ep 5 (Step 000645): Train loss 4.032, Val loss 5.405\n",
            "Ep 5 (Step 000650): Train loss 4.091, Val loss 5.339\n",
            "Ep 5 (Step 000655): Train loss 3.972, Val loss 5.500\n",
            "Ep 5 (Step 000660): Train loss 4.073, Val loss 5.683\n",
            "Ep 5 (Step 000665): Train loss 4.136, Val loss 5.501\n",
            "Ep 5 (Step 000670): Train loss 3.972, Val loss 5.398\n",
            "Ep 5 (Step 000675): Train loss 4.075, Val loss 5.583\n",
            "Ep 5 (Step 000680): Train loss 4.152, Val loss 5.552\n",
            "Ep 5 (Step 000685): Train loss 3.911, Val loss 5.354\n",
            "Ep 5 (Step 000690): Train loss 4.105, Val loss 5.459\n",
            "Ep 5 (Step 000695): Train loss 4.115, Val loss 5.496\n",
            "Ep 5 (Step 000700): Train loss 4.050, Val loss 5.477\n",
            "Ep 5 (Step 000705): Train loss 4.089, Val loss 5.486\n",
            "Ep 5 (Step 000710): Train loss 4.000, Val loss 5.413\n",
            "Ep 5 (Step 000715): Train loss 4.050, Val loss 5.317\n",
            "Ep 5 (Step 000720): Train loss 4.091, Val loss 5.475\n",
            "Ep 5 (Step 000725): Train loss 4.054, Val loss 5.572\n",
            "Ep 5 (Step 000730): Train loss 3.919, Val loss 5.555\n",
            "Ep 5 (Step 000735): Train loss 4.055, Val loss 5.142\n",
            "Targaryen dynasty's at the Rhaenyra also known upon the Iron brought\n",
            "Ep 6 (Step 000740): Train loss 3.889, Val loss 5.355\n",
            "Ep 6 (Step 000745): Train loss 4.003, Val loss 5.481\n",
            "Ep 6 (Step 000750): Train loss 3.838, Val loss 5.604\n",
            "Ep 6 (Step 000755): Train loss 3.937, Val loss 5.709\n",
            "Ep 6 (Step 000760): Train loss 3.897, Val loss 5.310\n",
            "Ep 6 (Step 000765): Train loss 3.818, Val loss 5.638\n",
            "Ep 6 (Step 000770): Train loss 3.874, Val loss 5.573\n",
            "Ep 6 (Step 000775): Train loss 4.034, Val loss 5.562\n",
            "Ep 6 (Step 000780): Train loss 3.963, Val loss 5.497\n",
            "Ep 6 (Step 000785): Train loss 3.872, Val loss 5.409\n",
            "Ep 6 (Step 000790): Train loss 3.979, Val loss 5.624\n",
            "Ep 6 (Step 000795): Train loss 3.833, Val loss 5.649\n",
            "Ep 6 (Step 000800): Train loss 3.607, Val loss 5.323\n",
            "Ep 6 (Step 000805): Train loss 3.705, Val loss 5.702\n",
            "Ep 6 (Step 000810): Train loss 3.781, Val loss 5.519\n",
            "Ep 6 (Step 000815): Train loss 3.852, Val loss 5.604\n",
            "Ep 6 (Step 000820): Train loss 3.931, Val loss 5.482\n",
            "Ep 6 (Step 000825): Train loss 3.904, Val loss 5.465\n",
            "Ep 6 (Step 000830): Train loss 3.719, Val loss 5.415\n",
            "Ep 6 (Step 000835): Train loss 3.875, Val loss 5.707\n",
            "Ep 6 (Step 000840): Train loss 3.859, Val loss 5.428\n",
            "Ep 6 (Step 000845): Train loss 3.807, Val loss 5.453\n",
            "Ep 6 (Step 000850): Train loss 3.837, Val loss 5.295\n",
            "Ep 6 (Step 000855): Train loss 3.885, Val loss 5.473\n",
            "Ep 6 (Step 000860): Train loss 3.755, Val loss 5.366\n",
            "Ep 6 (Step 000865): Train loss 3.826, Val loss 5.535\n",
            "Ep 6 (Step 000870): Train loss 3.724, Val loss 5.461\n",
            "Ep 6 (Step 000875): Train loss 3.724, Val loss 5.466\n",
            "Ep 6 (Step 000880): Train loss 3.742, Val loss 5.735\n",
            "Ep 6 (Step 000885): Train loss 3.724, Val loss 5.239\n",
            "Targaryen dynasty's, was still many dared bum for Rhaerys ( \n",
            "Ep 7 (Step 000890): Train loss 3.668, Val loss 5.364\n",
            "Ep 7 (Step 000895): Train loss 3.750, Val loss 5.504\n",
            "Ep 7 (Step 000900): Train loss 3.646, Val loss 5.461\n",
            "Ep 7 (Step 000905): Train loss 3.653, Val loss 5.394\n",
            "Ep 7 (Step 000910): Train loss 3.565, Val loss 5.861\n",
            "Ep 7 (Step 000915): Train loss 3.718, Val loss 5.705\n",
            "Ep 7 (Step 000920): Train loss 3.601, Val loss 5.653\n",
            "Ep 7 (Step 000925): Train loss 3.651, Val loss 5.767\n",
            "Ep 7 (Step 000930): Train loss 3.669, Val loss 5.541\n",
            "Ep 7 (Step 000935): Train loss 3.645, Val loss 5.617\n",
            "Ep 7 (Step 000940): Train loss 3.502, Val loss 5.646\n",
            "Ep 7 (Step 000945): Train loss 3.518, Val loss 5.745\n",
            "Ep 7 (Step 000950): Train loss 3.629, Val loss 5.844\n",
            "Ep 7 (Step 000955): Train loss 3.667, Val loss 5.620\n",
            "Ep 7 (Step 000960): Train loss 3.619, Val loss 5.420\n",
            "Ep 7 (Step 000965): Train loss 3.635, Val loss 5.645\n",
            "Ep 7 (Step 000970): Train loss 3.629, Val loss 5.500\n",
            "Ep 7 (Step 000975): Train loss 3.562, Val loss 5.554\n",
            "Ep 7 (Step 000980): Train loss 3.547, Val loss 5.576\n",
            "Ep 7 (Step 000985): Train loss 3.654, Val loss 5.595\n",
            "Ep 7 (Step 000990): Train loss 3.546, Val loss 5.482\n",
            "Ep 7 (Step 000995): Train loss 3.648, Val loss 5.565\n",
            "Ep 7 (Step 001000): Train loss 3.772, Val loss 5.522\n",
            "Ep 7 (Step 001005): Train loss 3.644, Val loss 5.877\n",
            "Ep 7 (Step 001010): Train loss 3.687, Val loss 5.641\n",
            "Ep 7 (Step 001015): Train loss 3.532, Val loss 5.590\n",
            "Ep 7 (Step 001020): Train loss 3.560, Val loss 5.613\n",
            "Ep 7 (Step 001025): Train loss 3.466, Val loss 5.379\n",
            "Ep 7 (Step 001030): Train loss 3.617, Val loss 5.716\n",
            "Ep 7 (Step 001035): Train loss 3.375, Val loss 5.512\n",
            "Targaryen dynasty's when Lord L Stark, in a third man. “I want.\n",
            "Ep 8 (Step 001040): Train loss 3.458, Val loss 5.422\n",
            "Ep 8 (Step 001045): Train loss 3.533, Val loss 5.596\n",
            "Ep 8 (Step 001050): Train loss 3.284, Val loss 5.550\n",
            "Ep 8 (Step 001055): Train loss 3.323, Val loss 5.545\n",
            "Ep 8 (Step 001060): Train loss 3.401, Val loss 5.629\n",
            "Ep 8 (Step 001065): Train loss 3.458, Val loss 5.754\n",
            "Ep 8 (Step 001070): Train loss 3.304, Val loss 5.654\n",
            "Ep 8 (Step 001075): Train loss 3.422, Val loss 5.780\n",
            "Ep 8 (Step 001080): Train loss 3.190, Val loss 5.952\n",
            "Ep 8 (Step 001085): Train loss 3.357, Val loss 5.803\n",
            "Ep 8 (Step 001090): Train loss 3.413, Val loss 5.800\n",
            "Ep 8 (Step 001095): Train loss 3.323, Val loss 5.725\n",
            "Ep 8 (Step 001100): Train loss 3.444, Val loss 5.841\n",
            "Ep 8 (Step 001105): Train loss 3.443, Val loss 5.771\n",
            "Ep 8 (Step 001110): Train loss 3.399, Val loss 5.703\n",
            "Ep 8 (Step 001115): Train loss 3.412, Val loss 5.688\n",
            "Ep 8 (Step 001120): Train loss 3.258, Val loss 5.728\n",
            "Ep 8 (Step 001125): Train loss 3.193, Val loss 5.797\n",
            "Ep 8 (Step 001130): Train loss 3.424, Val loss 5.564\n",
            "Ep 8 (Step 001135): Train loss 3.466, Val loss 5.809\n",
            "Ep 8 (Step 001140): Train loss 3.398, Val loss 5.839\n",
            "Ep 8 (Step 001145): Train loss 3.389, Val loss 5.761\n",
            "Ep 8 (Step 001150): Train loss 3.343, Val loss 5.581\n",
            "Ep 8 (Step 001155): Train loss 3.336, Val loss 5.735\n",
            "Ep 8 (Step 001160): Train loss 3.359, Val loss 5.695\n",
            "Ep 8 (Step 001165): Train loss 3.330, Val loss 5.497\n",
            "Ep 8 (Step 001170): Train loss 3.352, Val loss 5.624\n",
            "Ep 8 (Step 001175): Train loss 3.387, Val loss 5.521\n",
            "Ep 8 (Step 001180): Train loss 3.236, Val loss 5.641\n",
            "Targaryen dynasty's her mother as a calum again and beyond the small brown knights would she\n",
            "Ep 9 (Step 001185): Train loss 3.216, Val loss 5.571\n",
            "Ep 9 (Step 001190): Train loss 3.126, Val loss 5.613\n",
            "Ep 9 (Step 001195): Train loss 3.327, Val loss 6.005\n",
            "Ep 9 (Step 001200): Train loss 3.120, Val loss 5.953\n",
            "Ep 9 (Step 001205): Train loss 2.977, Val loss 5.966\n",
            "Ep 9 (Step 001210): Train loss 3.130, Val loss 5.855\n",
            "Ep 9 (Step 001215): Train loss 3.257, Val loss 5.794\n",
            "Ep 9 (Step 001220): Train loss 3.183, Val loss 5.760\n",
            "Ep 9 (Step 001225): Train loss 3.146, Val loss 5.845\n",
            "Ep 9 (Step 001230): Train loss 3.285, Val loss 5.889\n",
            "Ep 9 (Step 001235): Train loss 2.907, Val loss 5.812\n",
            "Ep 9 (Step 001240): Train loss 3.234, Val loss 5.896\n",
            "Ep 9 (Step 001245): Train loss 3.034, Val loss 6.163\n",
            "Ep 9 (Step 001250): Train loss 2.962, Val loss 6.010\n",
            "Ep 9 (Step 001255): Train loss 3.082, Val loss 5.960\n",
            "Ep 9 (Step 001260): Train loss 3.207, Val loss 6.034\n",
            "Ep 9 (Step 001265): Train loss 3.128, Val loss 5.985\n",
            "Ep 9 (Step 001270): Train loss 3.203, Val loss 5.873\n",
            "Ep 9 (Step 001275): Train loss 3.105, Val loss 6.032\n",
            "Ep 9 (Step 001280): Train loss 3.027, Val loss 5.850\n",
            "Ep 9 (Step 001285): Train loss 3.137, Val loss 5.960\n",
            "Ep 9 (Step 001290): Train loss 3.043, Val loss 6.083\n",
            "Ep 9 (Step 001295): Train loss 3.140, Val loss 5.722\n",
            "Ep 9 (Step 001300): Train loss 2.928, Val loss 5.879\n",
            "Ep 9 (Step 001305): Train loss 3.305, Val loss 5.939\n",
            "Ep 9 (Step 001310): Train loss 2.944, Val loss 5.763\n",
            "Ep 9 (Step 001315): Train loss 3.010, Val loss 5.833\n",
            "Ep 9 (Step 001320): Train loss 3.044, Val loss 5.773\n",
            "Ep 9 (Step 001325): Train loss 3.029, Val loss 5.846\n",
            "Ep 9 (Step 001330): Train loss 2.960, Val loss 5.715\n",
            "Targaryen dynasty'sy-natestone, named his horse. Grand his men relating,\n",
            "Ep 10 (Step 001335): Train loss 2.895, Val loss 5.830\n",
            "Ep 10 (Step 001340): Train loss 2.922, Val loss 6.158\n",
            "Ep 10 (Step 001345): Train loss 2.816, Val loss 6.000\n",
            "Ep 10 (Step 001350): Train loss 2.997, Val loss 5.978\n",
            "Ep 10 (Step 001355): Train loss 2.867, Val loss 5.952\n",
            "Ep 10 (Step 001360): Train loss 2.478, Val loss 6.224\n",
            "Ep 10 (Step 001365): Train loss 2.773, Val loss 6.091\n",
            "Ep 10 (Step 001370): Train loss 2.948, Val loss 6.243\n",
            "Ep 10 (Step 001375): Train loss 2.921, Val loss 6.519\n",
            "Ep 10 (Step 001380): Train loss 2.819, Val loss 6.219\n",
            "Ep 10 (Step 001385): Train loss 2.847, Val loss 5.974\n",
            "Ep 10 (Step 001390): Train loss 2.763, Val loss 5.977\n",
            "Ep 10 (Step 001395): Train loss 2.779, Val loss 6.015\n",
            "Ep 10 (Step 001400): Train loss 2.711, Val loss 6.071\n",
            "Ep 10 (Step 001405): Train loss 2.766, Val loss 5.976\n",
            "Ep 10 (Step 001410): Train loss 2.624, Val loss 6.292\n",
            "Ep 10 (Step 001415): Train loss 2.628, Val loss 6.163\n",
            "Ep 10 (Step 001420): Train loss 2.717, Val loss 6.021\n",
            "Ep 10 (Step 001425): Train loss 3.049, Val loss 5.955\n",
            "Ep 10 (Step 001430): Train loss 2.909, Val loss 6.139\n",
            "Ep 10 (Step 001435): Train loss 2.671, Val loss 5.882\n",
            "Ep 10 (Step 001440): Train loss 2.669, Val loss 6.254\n",
            "Ep 10 (Step 001445): Train loss 2.624, Val loss 6.009\n",
            "Ep 10 (Step 001450): Train loss 2.828, Val loss 6.209\n",
            "Ep 10 (Step 001455): Train loss 2.652, Val loss 6.222\n",
            "Ep 10 (Step 001460): Train loss 2.800, Val loss 6.086\n",
            "Ep 10 (Step 001465): Train loss 2.613, Val loss 6.010\n",
            "Ep 10 (Step 001470): Train loss 2.710, Val loss 6.016\n",
            "Ep 10 (Step 001475): Train loss 2.664, Val loss 5.892\n",
            "Targaryen dynasty's went the caught up of House, as well have V\n",
            "Ep 11 (Step 001480): Train loss 2.673, Val loss 5.917\n",
            "Ep 11 (Step 001485): Train loss 2.551, Val loss 5.975\n",
            "Ep 11 (Step 001490): Train loss 2.552, Val loss 6.080\n",
            "Ep 11 (Step 001495): Train loss 2.680, Val loss 6.180\n",
            "Ep 11 (Step 001500): Train loss 2.521, Val loss 6.145\n",
            "Ep 11 (Step 001505): Train loss 2.560, Val loss 6.267\n",
            "Ep 11 (Step 001510): Train loss 2.400, Val loss 6.202\n",
            "Ep 11 (Step 001515): Train loss 2.537, Val loss 6.434\n",
            "Ep 11 (Step 001520): Train loss 2.717, Val loss 6.219\n",
            "Ep 11 (Step 001525): Train loss 2.351, Val loss 6.310\n",
            "Ep 11 (Step 001530): Train loss 2.615, Val loss 6.166\n",
            "Ep 11 (Step 001535): Train loss 2.634, Val loss 6.372\n",
            "Ep 11 (Step 001540): Train loss 2.497, Val loss 6.454\n",
            "Ep 11 (Step 001545): Train loss 2.404, Val loss 6.333\n",
            "Ep 11 (Step 001550): Train loss 2.310, Val loss 6.250\n",
            "Ep 11 (Step 001555): Train loss 2.521, Val loss 6.154\n",
            "Ep 11 (Step 001560): Train loss 2.589, Val loss 6.336\n",
            "Ep 11 (Step 001565): Train loss 2.622, Val loss 6.187\n",
            "Ep 11 (Step 001570): Train loss 2.421, Val loss 6.281\n",
            "Ep 11 (Step 001575): Train loss 2.553, Val loss 6.126\n",
            "Ep 11 (Step 001580): Train loss 2.535, Val loss 6.254\n",
            "Ep 11 (Step 001585): Train loss 2.401, Val loss 6.132\n",
            "Ep 11 (Step 001590): Train loss 2.373, Val loss 6.205\n",
            "Ep 11 (Step 001595): Train loss 2.522, Val loss 6.094\n",
            "Ep 11 (Step 001600): Train loss 2.378, Val loss 6.313\n",
            "Ep 11 (Step 001605): Train loss 2.391, Val loss 6.256\n",
            "Ep 11 (Step 001610): Train loss 2.338, Val loss 6.153\n",
            "Ep 11 (Step 001615): Train loss 2.329, Val loss 6.293\n",
            "Ep 11 (Step 001620): Train loss 2.358, Val loss 6.257\n",
            "Ep 11 (Step 001625): Train loss 2.265, Val loss 6.193\n",
            "Targaryen dynasty's AC, with the bless, and a measure of small islands\n",
            "Ep 12 (Step 001630): Train loss 2.254, Val loss 6.302\n",
            "Ep 12 (Step 001635): Train loss 2.231, Val loss 6.377\n",
            "Ep 12 (Step 001640): Train loss 2.349, Val loss 6.408\n",
            "Ep 12 (Step 001645): Train loss 2.133, Val loss 6.494\n",
            "Ep 12 (Step 001650): Train loss 2.190, Val loss 6.420\n",
            "Ep 12 (Step 001655): Train loss 2.345, Val loss 6.540\n",
            "Ep 12 (Step 001660): Train loss 2.329, Val loss 6.404\n",
            "Ep 12 (Step 001665): Train loss 2.129, Val loss 6.303\n",
            "Ep 12 (Step 001670): Train loss 2.107, Val loss 6.441\n",
            "Ep 12 (Step 001675): Train loss 2.293, Val loss 6.406\n",
            "Ep 12 (Step 001680): Train loss 2.281, Val loss 6.492\n",
            "Ep 12 (Step 001685): Train loss 2.458, Val loss 6.564\n",
            "Ep 12 (Step 001690): Train loss 2.111, Val loss 6.695\n",
            "Ep 12 (Step 001695): Train loss 2.058, Val loss 6.414\n",
            "Ep 12 (Step 001700): Train loss 2.182, Val loss 6.394\n",
            "Ep 12 (Step 001705): Train loss 2.149, Val loss 6.450\n",
            "Ep 12 (Step 001710): Train loss 2.232, Val loss 7.035\n",
            "Ep 12 (Step 001715): Train loss 2.064, Val loss 6.574\n",
            "Ep 12 (Step 001720): Train loss 2.212, Val loss 6.647\n",
            "Ep 12 (Step 001725): Train loss 2.172, Val loss 6.299\n",
            "Ep 12 (Step 001730): Train loss 2.085, Val loss 6.625\n",
            "Ep 12 (Step 001735): Train loss 1.971, Val loss 6.269\n",
            "Ep 12 (Step 001740): Train loss 2.123, Val loss 6.660\n",
            "Ep 12 (Step 001745): Train loss 1.989, Val loss 6.734\n",
            "Ep 12 (Step 001750): Train loss 2.052, Val loss 6.550\n",
            "Ep 12 (Step 001755): Train loss 2.090, Val loss 6.709\n",
            "Ep 12 (Step 001760): Train loss 1.903, Val loss 6.585\n",
            "Ep 12 (Step 001765): Train loss 1.923, Val loss 6.505\n",
            "Ep 12 (Step 001770): Train loss 2.001, Val loss 6.254\n",
            "Ep 12 (Step 001775): Train loss 1.946, Val loss 6.328\n",
            "Targaryen dynasty's. not quite were not quite ahead? “I shall have\n",
            "Ep 13 (Step 001780): Train loss 1.854, Val loss 6.514\n",
            "Ep 13 (Step 001785): Train loss 1.909, Val loss 6.386\n",
            "Ep 13 (Step 001790): Train loss 2.000, Val loss 6.751\n",
            "Ep 13 (Step 001795): Train loss 2.062, Val loss 6.545\n",
            "Ep 13 (Step 001800): Train loss 1.904, Val loss 6.981\n",
            "Ep 13 (Step 001805): Train loss 1.580, Val loss 6.604\n",
            "Ep 13 (Step 001810): Train loss 1.916, Val loss 6.814\n",
            "Ep 13 (Step 001815): Train loss 1.726, Val loss 6.609\n",
            "Ep 13 (Step 001820): Train loss 1.666, Val loss 6.557\n",
            "Ep 13 (Step 001825): Train loss 1.812, Val loss 6.878\n",
            "Ep 13 (Step 001830): Train loss 1.953, Val loss 6.643\n",
            "Ep 13 (Step 001835): Train loss 1.822, Val loss 6.659\n",
            "Ep 13 (Step 001840): Train loss 1.781, Val loss 6.667\n",
            "Ep 13 (Step 001845): Train loss 1.661, Val loss 6.780\n",
            "Ep 13 (Step 001850): Train loss 1.841, Val loss 6.703\n",
            "Ep 13 (Step 001855): Train loss 1.805, Val loss 6.649\n",
            "Ep 13 (Step 001860): Train loss 1.593, Val loss 6.781\n",
            "Ep 13 (Step 001865): Train loss 1.513, Val loss 6.858\n",
            "Ep 13 (Step 001870): Train loss 1.893, Val loss 6.815\n",
            "Ep 13 (Step 001875): Train loss 1.805, Val loss 6.709\n",
            "Ep 13 (Step 001880): Train loss 1.726, Val loss 6.855\n",
            "Ep 13 (Step 001885): Train loss 1.658, Val loss 6.849\n",
            "Ep 13 (Step 001890): Train loss 1.797, Val loss 6.754\n",
            "Ep 13 (Step 001895): Train loss 1.559, Val loss 6.910\n",
            "Ep 13 (Step 001900): Train loss 1.790, Val loss 6.613\n",
            "Ep 13 (Step 001905): Train loss 1.668, Val loss 6.955\n",
            "Ep 13 (Step 001910): Train loss 1.674, Val loss 6.756\n",
            "Ep 13 (Step 001915): Train loss 1.645, Val loss 6.853\n",
            "Ep 13 (Step 001920): Train loss 1.630, Val loss 6.970\n",
            "Targaryen dynasty's in and a young had wed, was on Dragonstone was a \n",
            "Ep 14 (Step 001925): Train loss 1.589, Val loss 7.001\n",
            "Ep 14 (Step 001930): Train loss 1.581, Val loss 6.920\n",
            "Ep 14 (Step 001935): Train loss 1.475, Val loss 6.839\n",
            "Ep 14 (Step 001940): Train loss 1.698, Val loss 7.297\n",
            "Ep 14 (Step 001945): Train loss 1.439, Val loss 6.994\n",
            "Ep 14 (Step 001950): Train loss 1.545, Val loss 6.966\n",
            "Ep 14 (Step 001955): Train loss 1.580, Val loss 7.137\n",
            "Ep 14 (Step 001960): Train loss 1.481, Val loss 6.768\n",
            "Ep 14 (Step 001965): Train loss 1.411, Val loss 7.216\n",
            "Ep 14 (Step 001970): Train loss 1.396, Val loss 7.119\n",
            "Ep 14 (Step 001975): Train loss 1.492, Val loss 7.255\n",
            "Ep 14 (Step 001980): Train loss 1.424, Val loss 7.486\n",
            "Ep 14 (Step 001985): Train loss 1.494, Val loss 7.365\n",
            "Ep 14 (Step 001990): Train loss 1.558, Val loss 7.035\n",
            "Ep 14 (Step 001995): Train loss 1.387, Val loss 6.871\n",
            "Ep 14 (Step 002000): Train loss 1.629, Val loss 6.978\n",
            "Ep 14 (Step 002005): Train loss 1.371, Val loss 6.981\n",
            "Ep 14 (Step 002010): Train loss 1.561, Val loss 7.034\n",
            "Ep 14 (Step 002015): Train loss 1.433, Val loss 7.122\n",
            "Ep 14 (Step 002020): Train loss 1.424, Val loss 7.049\n",
            "Ep 14 (Step 002025): Train loss 1.502, Val loss 7.105\n",
            "Ep 14 (Step 002030): Train loss 1.396, Val loss 6.883\n",
            "Ep 14 (Step 002035): Train loss 1.487, Val loss 7.165\n",
            "Ep 14 (Step 002040): Train loss 1.457, Val loss 6.944\n",
            "Ep 14 (Step 002045): Train loss 1.280, Val loss 7.112\n",
            "Ep 14 (Step 002050): Train loss 1.323, Val loss 7.275\n",
            "Ep 14 (Step 002055): Train loss 1.320, Val loss 7.454\n",
            "Ep 14 (Step 002060): Train loss 1.346, Val loss 7.195\n",
            "Ep 14 (Step 002065): Train loss 1.317, Val loss 7.007\n",
            "Ep 14 (Step 002070): Train loss 1.316, Val loss 6.933\n",
            "Targaryen dynasty's than Though the notion was two years, the newborn lords always, King\n",
            "Ep 15 (Step 002075): Train loss 1.285, Val loss 6.850\n",
            "Ep 15 (Step 002080): Train loss 1.292, Val loss 7.360\n",
            "Ep 15 (Step 002085): Train loss 1.121, Val loss 7.267\n",
            "Ep 15 (Step 002090): Train loss 1.229, Val loss 7.181\n",
            "Ep 15 (Step 002095): Train loss 1.166, Val loss 7.391\n",
            "Ep 15 (Step 002100): Train loss 1.143, Val loss 7.123\n",
            "Ep 15 (Step 002105): Train loss 1.168, Val loss 7.385\n",
            "Ep 15 (Step 002110): Train loss 1.251, Val loss 7.393\n",
            "Ep 15 (Step 002115): Train loss 1.177, Val loss 7.318\n",
            "Ep 15 (Step 002120): Train loss 1.220, Val loss 7.345\n",
            "Ep 15 (Step 002125): Train loss 1.222, Val loss 7.279\n",
            "Ep 15 (Step 002130): Train loss 1.115, Val loss 7.600\n",
            "Ep 15 (Step 002135): Train loss 1.219, Val loss 7.679\n",
            "Ep 15 (Step 002140): Train loss 1.255, Val loss 7.661\n",
            "Ep 15 (Step 002145): Train loss 1.008, Val loss 7.536\n",
            "Ep 15 (Step 002150): Train loss 1.161, Val loss 7.713\n",
            "Ep 15 (Step 002155): Train loss 1.099, Val loss 7.330\n",
            "Ep 15 (Step 002160): Train loss 1.130, Val loss 7.549\n",
            "Ep 15 (Step 002165): Train loss 1.256, Val loss 7.489\n",
            "Ep 15 (Step 002170): Train loss 1.112, Val loss 7.635\n",
            "Ep 15 (Step 002175): Train loss 1.122, Val loss 7.187\n",
            "Ep 15 (Step 002180): Train loss 1.219, Val loss 7.464\n",
            "Ep 15 (Step 002185): Train loss 1.108, Val loss 7.563\n",
            "Ep 15 (Step 002190): Train loss 0.992, Val loss 7.325\n",
            "Ep 15 (Step 002195): Train loss 0.911, Val loss 7.162\n",
            "Ep 15 (Step 002200): Train loss 1.193, Val loss 7.495\n",
            "Ep 15 (Step 002205): Train loss 0.955, Val loss 7.659\n",
            "Ep 15 (Step 002210): Train loss 1.038, Val loss 7.406\n",
            "Ep 15 (Step 002215): Train loss 0.997, Val loss 7.487\n",
            "Targaryen dynasty's at last much pleased him as often as Vhagar. in his\n",
            "Training completed in 44.21 minutes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "\n",
        "\n",
        "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
        "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
        "\n",
        "    # Plot training and validation loss against epochs\n",
        "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
        "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
        "    ax1.set_xlabel(\"Epochs\")\n",
        "    ax1.set_ylabel(\"Loss\")\n",
        "    ax1.legend(loc=\"upper right\")\n",
        "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
        "\n",
        "    # Create a second x-axis for tokens seen\n",
        "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
        "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
        "    ax2.set_xlabel(\"Tokens seen\")\n",
        "\n",
        "    fig.tight_layout()  # Adjust layout to make room\n",
        "    plt.savefig(\"loss-plot.pdf\")\n",
        "    plt.show()\n",
        "\n",
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
        "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "FE7bgEZ0WpNz",
        "outputId": "6d0683fb-5931-47f8-a014-254d5af4e06c"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x300 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}